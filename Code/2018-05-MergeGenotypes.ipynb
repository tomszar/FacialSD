{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging genotypes\n",
    "\n",
    "This script will merge several genotypes files\n",
    "- Merging between participants' genotype platforms\n",
    "- Merging reference samples with participants' samples\n",
    "\n",
    "For reference samples we will use the 1000G and HGDP samples. \n",
    "To see the steps to merge the reference samples see [here](https://tomszar.github.io/HGDP_1000G_Merge/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "First let's import modules and set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, shutil, subprocess, csv, time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "projpath  = os.path.realpath('..')\n",
    "pathgenos = os.path.join(projpath, \"DataBases\", \"Genotypes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging in house genotypes\n",
    "\n",
    "The first step will be to merge our participants' genotypes. \n",
    "The steps will be as follows:\n",
    "- Clean each dataset by removing SNPs with missing rates greater than 0.1, SNPs with minor allele frequencies below 0.01, and a Hardy-Weinberg equilibrium exact test p-value below 1e-50\n",
    "- Merge the datasets, flip strands, and remove possibly triallelic SNPs\n",
    "- Finally, LD prune SNPs in the merged dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will do a cleaning in each data set to remove SNP with missing call rates greater than 0.1, minor allele frequencies below 0.01, and a Hardy-Weinberg equilibrium exact test p-value below 1e-50. \n",
    "We'll also remove individuals without sex (those are duplicated and controls from plates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move directory\n",
    "os.chdir(pathgenos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating...Clean_Euro180_176ppl_317K_hg19_ATGC\n",
      "Creating...Clean_GHPAFF_3ppl_907K_hg19_ATGC\n",
      "Creating...Clean_ADAPT_2784ppl_1Msnps_hg19_ATGC\n",
      "Creating...Clean_SA_231ppl_599K_hg19_ATGC\n",
      "Creating...Clean_TD_198ppl_1M_hg19_ATGC\n",
      "Creating...Clean_CV_697ppl_964K_hg19_ATGC\n",
      "Creating...Clean_CHP_1022ppl_114K_hg19_ATGC\n",
      "Creating...Clean_UIUC2014_168ppl_703K_hg19_ATGC\n",
      "Creating...Clean_UIUC2013_116ppl_959Ksnps_hg19_ATGC\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#Clean datasets\n",
    "for file in glob.glob(\"01_Originals/*.bed\"):\n",
    "    inputname = file.split(\".\")\n",
    "    outname = \"Clean_\" + inputname[0][13:]\n",
    "    print(\"Creating...\" + outname)\n",
    "    subprocess.run([\"plink\", \"--bfile\", inputname[0], \"--geno\", \"0.1\", \"--maf\", \"0.01\", \"--hwe\", \"1e-50\", \"--make-bed\", \"--out\", \"02_Cleaning/\" + outname])\n",
    "\n",
    "    \n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can take a look at the loaded and removed SNPs in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file: Clean_TD_198ppl_1M_hg19_ATGC\n",
      "1032848 variants loaded from .bim file.\n",
      "454245 variants removed due to missing genotype data (--geno).\n",
      "57492 variants removed due to minor allele threshold(s)\n",
      "521110 variants and 198 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_SA_231ppl_599K_hg19_ATGC\n",
      "599855 variants loaded from .bim file.\n",
      "25249 variants removed due to missing genotype data (--geno).\n",
      "--hwe: 5 variants removed due to Hardy-Weinberg exact test.\n",
      "48377 variants removed due to minor allele threshold(s)\n",
      "526224 variants and 231 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_GHPAFF_3ppl_907K_hg19_ATGC\n",
      "907494 variants loaded from .bim file.\n",
      "52823 variants removed due to missing genotype data (--geno).\n",
      "chromosome variants.\n",
      "--hwe: 0 variants removed due to Hardy-Weinberg exact test.\n",
      "376349 variants removed due to minor allele threshold(s)\n",
      "478322 variants and 3 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_Euro180_176ppl_317K_hg19_ATGC\n",
      "317503 variants loaded from .bim file.\n",
      "3416 variants removed due to missing genotype data (--geno).\n",
      "--hwe: 0 variants removed due to Hardy-Weinberg exact test.\n",
      "324 variants removed due to minor allele threshold(s)\n",
      "313763 variants and 176 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_UIUC2013_116ppl_959Ksnps_hg19_ATGC\n",
      "959382 variants loaded from .bim file.\n",
      "30151 variants removed due to missing genotype data (--geno).\n",
      "chromosome variants.\n",
      "--hwe: 0 variants removed due to Hardy-Weinberg exact test.\n",
      "52615 variants removed due to minor allele threshold(s)\n",
      "876616 variants and 116 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_UIUC2014_168ppl_703K_hg19_ATGC\n",
      "703725 variants loaded from .bim file.\n",
      "141590 variants removed due to missing genotype data (--geno).\n",
      "chromosome variants.\n",
      "--hwe: 0 variants removed due to Hardy-Weinberg exact test.\n",
      "46141 variants removed due to minor allele threshold(s)\n",
      "515994 variants and 168 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_ADAPT_2784ppl_1Msnps_hg19_ATGC\n",
      "1060559 variants loaded from .bim file.\n",
      "492757 variants removed due to missing genotype data (--geno).\n",
      "--hwe: 353 variants removed due to Hardy-Weinberg exact test.\n",
      "42302 variants removed due to minor allele threshold(s)\n",
      "525147 variants and 2784 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_CV_697ppl_964K_hg19_ATGC\n",
      "964041 variants loaded from .bim file.\n",
      "0 variants removed due to missing genotype data (--geno).\n",
      "chromosome variants.\n",
      "--hwe: 63 variants removed due to Hardy-Weinberg exact test.\n",
      "76129 variants removed due to minor allele threshold(s)\n",
      "887849 variants and 697 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_CHP_1022ppl_114K_hg19_ATGC\n",
      "114495 variants loaded from .bim file.\n",
      "1132 variants removed due to missing genotype data (--geno).\n",
      "--hwe: 0 variants removed due to Hardy-Weinberg exact test.\n",
      "753 variants removed due to minor allele threshold(s)\n",
      "112610 variants and 1022 people pass filters and QC.\n",
      "Finished file... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in glob.glob(\"02_Cleaning/*.log\"):\n",
    "    with open(file) as myfile:\n",
    "        print(\"In file: \" + file.split(\".\")[0][12:])\n",
    "        for num, line in enumerate(myfile, 1):\n",
    "            if \"variants\" in line:\n",
    "                print(line, end='')\n",
    "        print(\"Finished file... \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will generate a first merging to get a list of problematic snps.\n",
    "Based on comparing a few snps across datasets, it seems that the CV dataset contains most fliped snps, followed by the Euro dataset.\n",
    "Then, we will flip the snps of the CV dataset, merge, flip the Euro dataset, and merge again.\n",
    "Finally, we'll then extract the possibly triallelic snps from each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85589 16206 3786\n",
      "Creating...CleanTriallelic_UIUC2014_168ppl_703K_hg19_ATGC\n",
      "Creating...CleanTriallelic_ADAPT_2784ppl_1Msnps_hg19_ATGC\n",
      "Creating...CleanTriallelic_CV_697ppl_964K_hg19_ATGC\n",
      "Creating...CleanTriallelic_CV_697ppl_964K_hg19_ATGC_flip\n",
      "Creating...CleanTriallelic_UIUC2013_116ppl_959Ksnps_hg19_ATGC\n",
      "Creating...CleanTriallelic_CHP_1022ppl_114K_hg19_ATGC\n",
      "Creating...CleanTriallelic_TD_198ppl_1M_hg19_ATGC\n",
      "Creating...CleanTriallelic_GHPAFF_3ppl_907K_hg19_ATGC\n",
      "Creating...CleanTriallelic_Euro180_176ppl_317K_hg19_ATGC\n",
      "Creating...CleanTriallelic_Euro180_176ppl_317K_hg19_ATGC_flip\n",
      "Creating...CleanTriallelic_SA_231ppl_599K_hg19_ATGC\n",
      "Finished\n",
      "Final merging\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['plink', '--merge-list', 'FinalMergeList.txt', '--out', '03_Merging/Merged'], returncode=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First merge\n",
    "subprocess.run([\"plink\", \"--merge-list\", \"FirstMergeList.txt\", \"--out\", \"03_Merging/TriallelicSnps\"])\n",
    "#Flip the CV dataset\n",
    "subprocess.run([\"plink\", \"--bfile\", \"02_Cleaning/Clean_CV_697ppl_964K_hg19_ATGC\", \"--flip\", \"03_Merging/TriallelicSnps.missnp\",\n",
    "                \"--make-bed\", \"--out\", \"02_Cleaning/Clean_CV_697ppl_964K_hg19_ATGC_flip\"])\n",
    "#Second merge\n",
    "subprocess.run([\"plink\", \"--merge-list\", \"SecondMergeList.txt\", \"--out\", \"03_Merging/TriallelicSnps_2\"])\n",
    "#Flip the Euro dataset\n",
    "subprocess.run([\"plink\", \"--bfile\", \"02_Cleaning/Clean_Euro180_176ppl_317K_hg19_ATGC\", \"--flip\", \"03_Merging/TriallelicSnps_2.missnp\",\n",
    "                \"--make-bed\", \"--out\", \"02_Cleaning/Clean_Euro180_176ppl_317K_hg19_ATGC_flip\"])\n",
    "#Third merge\n",
    "subprocess.run([\"plink\", \"--merge-list\", \"ThirdMergeList.txt\", \"--out\", \"03_Merging/TriallelicSnps_3\"])\n",
    "\n",
    "#Get number of snps missing from first to third merge\n",
    "num_lines  = sum(1 for line in open(\"03_Merging/TriallelicSnps.missnp\"))\n",
    "num_lines2 = sum(1 for line in open(\"03_Merging/TriallelicSnps_2.missnp\"))\n",
    "num_lines3 = sum(1 for line in open(\"03_Merging/TriallelicSnps_3.missnp\"))\n",
    "print(num_lines, num_lines2, num_lines3)\n",
    "\n",
    "#removing snps from TriallelicSnps_3.missnp files from all datasets\n",
    "for file in glob.glob(\"02_Cleaning/*.bed\"):\n",
    "    inputname = file.split(\".\")\n",
    "    outname = \"CleanTriallelic_\" + inputname[0][18:]\n",
    "    print(\"Creating...\" + outname)\n",
    "    subprocess.run([\"plink\", \"--bfile\", inputname[0], \"--exclude\", \"03_Merging/TriallelicSnps_3.missnp\", \"--make-bed\", \"--out\", \"02_Cleaning/\" + outname])\n",
    "    \n",
    "print(\"Finished\")\n",
    "print(\"Final merging\")\n",
    "subprocess.run([\"plink\", \"--merge-list\", \"FinalMergeList.txt\", \"--out\", \"03_Merging/Merged\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will remove all snps with missing call rates greater than 0.1, SNPs with minor allele frequencies below 0.01, and a Hardy-Weinberg equilibrium exact test p-value below 1e-50.\n",
    "We will also remove duplicated individuals (same IID but different FID), due to genotyping in two or more platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['plink', '--bfile', '04_CleanMerged/Cleaned', '--keep', '04_CleanMerged/keep_unique.fam', '--make-bed', '--out', '04_CleanMerged/Cleaned'], returncode=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"plink\", \"--bfile\", \"03_Merging/Merged\", \"--geno\", \"0.1\", \"--maf\", \"0.01\", \"--hwe\", \"1e-50\", \"--make-bed\", \"--out\", \"04_CleanMerged/Cleaned\"])\n",
    "fam    = pd.read_csv(\"04_CleanMerged/Cleaned.fam\", sep = \" \", header = None).drop_duplicates(subset = 1)\n",
    "fam.iloc[:,0:2].to_csv(\"04_CleanMerged/keep_unique.fam\", sep = \" \", header = False, index = False)\n",
    "subprocess.run([\"plink\", \"--bfile\", \"04_CleanMerged/Cleaned\", \"--keep\", \"04_CleanMerged/keep_unique.fam\", \"--make-bed\", \"--out\", \"04_CleanMerged/Cleaned\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will LD prune the set of SNPs using parameters 50, 5, and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29614 variants loaded from .bim file.\n",
      "29614 variants and 5212 people pass filters and QC.\n",
      "Pruned 1061 variants from chromosome 1, leaving 1903.\n",
      "Pruned 667 variants from chromosome 2, leaving 1369.\n",
      "Pruned 652 variants from chromosome 3, leaving 1235.\n",
      "Pruned 259 variants from chromosome 4, leaving 619.\n",
      "Pruned 545 variants from chromosome 5, leaving 1198.\n",
      "Pruned 730 variants from chromosome 6, leaving 1163.\n",
      "Pruned 473 variants from chromosome 7, leaving 873.\n",
      "Pruned 670 variants from chromosome 8, leaving 1099.\n",
      "Pruned 428 variants from chromosome 9, leaving 778.\n",
      "Pruned 488 variants from chromosome 10, leaving 842.\n",
      "Pruned 787 variants from chromosome 11, leaving 1439.\n",
      "Pruned 406 variants from chromosome 12, leaving 839.\n",
      "Pruned 254 variants from chromosome 13, leaving 561.\n",
      "Pruned 654 variants from chromosome 14, leaving 1002.\n",
      "Pruned 428 variants from chromosome 15, leaving 777.\n",
      "Pruned 297 variants from chromosome 16, leaving 562.\n",
      "Pruned 402 variants from chromosome 17, leaving 755.\n",
      "Pruned 115 variants from chromosome 18, leaving 241.\n",
      "Pruned 120 variants from chromosome 19, leaving 436.\n",
      "Pruned 298 variants from chromosome 20, leaving 588.\n",
      "Pruned 356 variants from chromosome 21, leaving 711.\n",
      "Pruned 168 variants from chromosome 22, leaving 351.\n",
      "Pruned 0 variants from chromosome 23, leaving 15.\n",
      "Pruning complete.  10258 of 29614 variants removed.\n",
      "Finished file... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "subprocess.run([\"plink\", \"--bfile\", \"04_CleanMerged/Cleaned\", \"--indep\", \"50\", \"5\", \"2\", \"--out\", \"04_CleanMerged/ExtractSNPs\"])\n",
    "with open(\"04_CleanMerged/ExtractSNPs.log\") as myfile:\n",
    "    for num, line in enumerate(myfile, 1):\n",
    "        if \"variants\" in line:\n",
    "            print(line, end='')\n",
    "    print(\"Finished file... \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29614 variants loaded from .bim file.\n",
      "--extract: 19356 variants remaining.\n",
      "19356 variants and 5124 people pass filters and QC.\n",
      "Finished file... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "subprocess.run([\"plink\", \"--bfile\", \"04_CleanMerged/Cleaned\", \"--extract\", \"04_CleanMerged/ExtractSNPs.prune.in\", \"--remove\", \"04_CleanMerged/ExtractSNPs.nosex\", \n",
    "                \"--make-bed\", \"--out\", \"04_CleanMerged/LDCleanMerged\"])\n",
    "with open(\"04_CleanMerged/LDCleanMerged.log\") as myfile:\n",
    "    for num, line in enumerate(myfile, 1):\n",
    "        if \"variants\" in line:\n",
    "            print(line, end='')\n",
    "    print(\"Finished file... \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll export the `04_CleanMerged/Cleaned` dataset to vcf and then split the file into chromosomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create vcf file\n",
    "subprocess.run([\"plink\", \"--bfile\", \"04_CleanMerged/Cleaned\", \"--remove\", \"04_CleanMerged/ExtractSNPs.nosex\", \"--recode\", \"vcf-iid\", \"bgz\", \"--out\", \"04_CleanMerged/Cleaned\"])\n",
    "#Split by chromosome\n",
    "for i in range(1,23):\n",
    "    subprocess.run([\"vcftools\", \"--gzvcf\", \"04_CleanMerged/Cleaned.vcf.gz\", \"--chr\", str(i), \"--recode\", \"--recode-INFO-all\", \"--out\", \n",
    "                    \"04_CleanMerged/Split/Cleaned_split_\" + str(i)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll remove intermediary datasets to clear space, and move the final dataset to be latter merged with the reference samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove source files\n",
    "for f in glob.glob(\"02_Cleaning/*.*\"):\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in glob.glob(\"03_Merging/*.*\"):\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy final file to Merge1000G to be merged with the 1000Genomes samples\n",
    "dest_dir = os.path.join(pathgenos, '05_ReferenceSamples')\n",
    "for filename in glob.glob(\"04_CleanMerged/LDCleanMerged.*\"):\n",
    "    shutil.copy(filename, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section delete the merging of the reference samples, and skip to the merging of the reference with our samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging reference and in-house samples\n",
    "\n",
    "Finally, we will merge the in-house samples with the reference samples from HGDP and 1000G.\n",
    "To do that we will extract the snps from the in-house samples already pruned and cleaned.\n",
    "Follow these [steps](https://tomszar.github.io/HGDP_1000G_Merge/) to merge the reference samples. \n",
    "Paste the final file `hgdp1000ghg19` in the 05_ReferenceSamples folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLINK v1.90b4 64-bit (20 Mar 2017)\n",
      "Options in effect:\n",
      "  --bfile LDCleanMerged_subset_flip\n",
      "  --bmerge hgdp1000ghg19_subset\n",
      "  --make-bed\n",
      "  --out HouseHGDP1000Ghg19\n",
      "\n",
      "Hostname: lenovo910\n",
      "Working directory: /home/tomas/Documents/FacialSD/DataBases/Genotypes/05_ReferenceSamples\n",
      "Start time: Sun May 27 17:39:56 2018\n",
      "\n",
      "Random number seed: 1527457196\n",
      "7715 MB RAM detected; reserving 3857 MB for main workspace.\n",
      "5124 people loaded from LDCleanMerged_subset_flip.fam.\n",
      "3444 people to be merged from hgdp1000ghg19_subset.fam.\n",
      "Of these, 3444 are new, while 0 are present in the base dataset.\n",
      "16031 markers loaded from LDCleanMerged_subset_flip.bim.\n",
      "16031 markers to be merged from hgdp1000ghg19_subset.bim.\n",
      "Of these, 0 are new, while 16031 are present in the base dataset.\n",
      "Performing single-pass merge (8568 people, 16031 variants).\n",
      "Merged fileset written to HouseHGDP1000Ghg19-merge.bed +\n",
      "HouseHGDP1000Ghg19-merge.bim + HouseHGDP1000Ghg19-merge.fam .\n",
      "16031 variants loaded from .bim file.\n",
      "8568 people (3652 males, 4916 females) loaded from .fam.\n",
      "Using 1 thread (no multithreaded calculations invoked).\n",
      "Before main variant filters, 8445 founders and 123 nonfounders present.\n",
      "Calculating allele frequencies... done.\n",
      "Total genotyping rate is 0.987512.\n",
      "16031 variants and 8568 people pass filters and QC.\n",
      "Note: No phenotypes present.\n",
      "--make-bed to HouseHGDP1000Ghg19.bed + HouseHGDP1000Ghg19.bim +\n",
      "HouseHGDP1000Ghg19.fam ... done.\n",
      "\n",
      "End time: Sun May 27 17:39:56 2018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.join(pathgenos, \"05_ReferenceSamples\"))\n",
    "subprocess.run([\"plink\", \"--bfile\", \"hgdp1000ghg19\", \"--extract\", \"LDCleanMerged.bim\", \"--make-bed\", \"--out\", \"hgdp1000ghg19_subset\"])\n",
    "subprocess.run([\"plink\", \"--bfile\", \"LDCleanMerged\", \"--extract\", \"hgdp1000ghg19_subset.bim\", \"--make-bed\", \"--out\", \"LDCleanMerged_subset\"])\n",
    "subprocess.run([\"plink\", \"--bfile\", \"LDCleanMerged_subset\", \"--bmerge\", \"hgdp1000ghg19_subset\", \"--make-bed\", \"--out\", \"HouseHGDP1000Ghg19\"])\n",
    "#Fliping strand and merging\n",
    "subprocess.run([\"plink\", \"--bfile\", \"LDCleanMerged_subset\", \"--flip\", \"HouseHGDP1000Ghg19-merge.missnp\", \"--make-bed\", \"--out\", \"LDCleanMerged_subset_flip\"])\n",
    "subprocess.run([\"plink\", \"--bfile\", \"LDCleanMerged_subset_flip\", \"--bmerge\", \"hgdp1000ghg19_subset\", \"--make-bed\", \"--out\", \"HouseHGDP1000Ghg19\"])\n",
    "\n",
    "with open(\"HouseHGDP1000Ghg19.log\", 'r') as fin:\n",
    "    file_contents = fin.read()\n",
    "    print(file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population stratification\n",
    "\n",
    "Now, we will load the final dataset created before and run some populations stratification analyses (`PCA`, `MDS` and `ADMIXTURE`)\n",
    "\n",
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(pathgenos, \"05_ReferenceSamples\"))\n",
    "subprocess.run([\"plink\", \"--bfile\", \"HouseHGDP1000Ghg19\", \"--pca\", \"50\", \"--pca-cluster-names\", \"0\", \"--within\", \"hgdp1000ghg19.fam\", \"--out\", \"PCA\"])\n",
    "for file in glob.glob(\"PCA.*\"):\n",
    "    shutil.move(file, os.path.join(projpath, \"Results\", \"GenPCA\", file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
