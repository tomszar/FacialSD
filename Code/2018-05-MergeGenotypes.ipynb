{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging genotypes\n",
    "\n",
    "This script will merge several genotypes files\n",
    "- Merging between participants' genotype platforms\n",
    "- Merging reference samples with participants' samples\n",
    "\n",
    "For reference samples we will use the 1000G and HGDP samples. \n",
    "To see the steps to merge the reference samples see [here](https://tomszar.github.io/HGDP_1000G_Merge/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "First let's import modules and set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, shutil, subprocess, csv, time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "projpath  = os.path.realpath('..')\n",
    "pathgenos = os.path.join(projpath, \"DataBases\", \"Genotypes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging in house genotypes\n",
    "\n",
    "The first step will be to merge our participants' genotypes. \n",
    "The steps will be as follows:\n",
    "- Clean each dataset by removing SNPs with missing rates greater than 0.1, SNPs with minor allele frequencies below 0.01, and a Hardy-Weinberg equilibrium exact test p-value below 1e-50\n",
    "- Merge the datasets, flip strands, and remove possibly triallelic SNPs\n",
    "- Finally, LD prune SNPs in the merged dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will do a cleaning in each data set to remove SNP with missing call rates greater than 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating...Clean_UIUC2013_116ppl_959Ksnps_hg19_ATGC\n",
      "Creating...Clean_Euro180_176ppl_317K_hg19_ATGC\n",
      "Creating...Clean_CHP_1022ppl_114K_hg19_ATGC\n",
      "Creating...Clean_SA_231ppl_599K_hg19_ATGC\n",
      "Creating...Clean_TD_198ppl_1M_hg19_ATGC\n",
      "Creating...Clean_ADAPT_2784ppl_1Msnps_hg19_ATGC\n",
      "Creating...Clean_GHPAFF_3ppl_907K_hg19_ATGC\n",
      "Creating...Clean_CV_697ppl_964K_hg19_ATGC\n",
      "Creating...Clean_UIUC2014_168ppl_703K_hg19_ATGC\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#Clean datasets\n",
    "os.chdir(pathgenos)\n",
    "for file in glob.glob(\"01_Originals/*.bed\"):\n",
    "    inputname = file.split(\".\")\n",
    "    outname = \"Clean_\" + inputname[0][13:]\n",
    "    print(\"Creating...\" + outname)\n",
    "    subprocess.run([\"plink\", \"--bfile\", inputname[0], \"--geno\", \"0.1\", \"--maf\", \"0.01\", \"--hwe\", \"1e-50\", \"--make-bed\", \"--out\", \"02_Cleaning/\" + outname])\n",
    "    \n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can take a look at the loaded and removed SNPs in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file: Clean_CHP_1022ppl_114K_hg19_ATGC\n",
      "114495 variants loaded from .bim file.\n",
      "1132 variants removed due to missing genotype data (--geno).\n",
      "--hwe: 0 variants removed due to Hardy-Weinberg exact test.\n",
      "753 variants removed due to minor allele threshold(s)\n",
      "112610 variants and 1022 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_UIUC2013_116ppl_959Ksnps_hg19_ATGC\n",
      "959382 variants loaded from .bim file.\n",
      "30151 variants removed due to missing genotype data (--geno).\n",
      "chromosome variants.\n",
      "--hwe: 0 variants removed due to Hardy-Weinberg exact test.\n",
      "52615 variants removed due to minor allele threshold(s)\n",
      "876616 variants and 116 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_GHPAFF_3ppl_907K_hg19_ATGC\n",
      "907494 variants loaded from .bim file.\n",
      "52823 variants removed due to missing genotype data (--geno).\n",
      "chromosome variants.\n",
      "--hwe: 0 variants removed due to Hardy-Weinberg exact test.\n",
      "376349 variants removed due to minor allele threshold(s)\n",
      "478322 variants and 3 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_TD_198ppl_1M_hg19_ATGC\n",
      "1032848 variants loaded from .bim file.\n",
      "454245 variants removed due to missing genotype data (--geno).\n",
      "57492 variants removed due to minor allele threshold(s)\n",
      "521110 variants and 198 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_CV_697ppl_964K_hg19_ATGC\n",
      "964041 variants loaded from .bim file.\n",
      "0 variants removed due to missing genotype data (--geno).\n",
      "chromosome variants.\n",
      "--hwe: 63 variants removed due to Hardy-Weinberg exact test.\n",
      "76129 variants removed due to minor allele threshold(s)\n",
      "887849 variants and 697 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_UIUC2014_168ppl_703K_hg19_ATGC\n",
      "703725 variants loaded from .bim file.\n",
      "141590 variants removed due to missing genotype data (--geno).\n",
      "chromosome variants.\n",
      "--hwe: 0 variants removed due to Hardy-Weinberg exact test.\n",
      "46141 variants removed due to minor allele threshold(s)\n",
      "515994 variants and 168 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_Euro180_176ppl_317K_hg19_ATGC\n",
      "317503 variants loaded from .bim file.\n",
      "3416 variants removed due to missing genotype data (--geno).\n",
      "--hwe: 0 variants removed due to Hardy-Weinberg exact test.\n",
      "324 variants removed due to minor allele threshold(s)\n",
      "313763 variants and 176 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_SA_231ppl_599K_hg19_ATGC\n",
      "599855 variants loaded from .bim file.\n",
      "25249 variants removed due to missing genotype data (--geno).\n",
      "--hwe: 5 variants removed due to Hardy-Weinberg exact test.\n",
      "48377 variants removed due to minor allele threshold(s)\n",
      "526224 variants and 231 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_ADAPT_2784ppl_1Msnps_hg19_ATGC\n",
      "1060559 variants loaded from .bim file.\n",
      "492757 variants removed due to missing genotype data (--geno).\n",
      "--hwe: 353 variants removed due to Hardy-Weinberg exact test.\n",
      "42302 variants removed due to minor allele threshold(s)\n",
      "525147 variants and 2784 people pass filters and QC.\n",
      "Finished file... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in glob.glob(\"02_Cleaning/*.log\"):\n",
    "    with open(file) as myfile:\n",
    "        print(\"In file: \" + file.split(\".\")[0][12:])\n",
    "        for num, line in enumerate(myfile, 1):\n",
    "            if \"variants\" in line:\n",
    "                print(line, end='')\n",
    "        print(\"Finished file... \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will generate a first merging to get a list of problematic snps.\n",
    "Based on comparing a few snps across datasets, it seems that the CV dataset contains most fliped snps, followed by the Euro dataset.\n",
    "Then, we will flip the snps of the CV dataset, merge, flip the Euro dataset, and merge again.\n",
    "Finally, we'll then extract the possibly triallelic snps from each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85589 16206 3786\n",
      "Creating...CleanTriallelic_TD_198ppl_1M_hg19_ATGC\n",
      "Creating...CleanTriallelic_GHPAFF_3ppl_907K_hg19_ATGC\n",
      "Creating...CleanTriallelic_Euro180_176ppl_317K_hg19_ATGC\n",
      "Creating...CleanTriallelic_UIUC2014_168ppl_703K_hg19_ATGC\n",
      "Creating...CleanTriallelic_CV_697ppl_964K_hg19_ATGC\n",
      "Creating...CleanTriallelic_Euro180_176ppl_317K_hg19_ATGC_flip\n",
      "Creating...CleanTriallelic_SA_231ppl_599K_hg19_ATGC\n",
      "Creating...CleanTriallelic_ADAPT_2784ppl_1Msnps_hg19_ATGC\n",
      "Creating...CleanTriallelic_CHP_1022ppl_114K_hg19_ATGC\n",
      "Creating...CleanTriallelic_CV_697ppl_964K_hg19_ATGC_flip\n",
      "Creating...CleanTriallelic_UIUC2013_116ppl_959Ksnps_hg19_ATGC\n",
      "Finished\n",
      "Final merging\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['plink', '--merge-list', 'FinalMergeList.txt', '--out', '03_Merging/Merged'], returncode=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First merge\n",
    "subprocess.run([\"plink\", \"--merge-list\", \"FirstMergeList.txt\", \"--out\", \"03_Merging/TriallelicSnps\"])\n",
    "#Flip the CV dataset\n",
    "subprocess.run([\"plink\", \"--bfile\", \"02_Cleaning/Clean_CV_697ppl_964K_hg19_ATGC\", \"--flip\", \"03_Merging/TriallelicSnps.missnp\",\n",
    "                \"--make-bed\", \"--out\", \"02_Cleaning/Clean_CV_697ppl_964K_hg19_ATGC_flip\"])\n",
    "#Second merge\n",
    "subprocess.run([\"plink\", \"--merge-list\", \"SecondMergeList.txt\", \"--out\", \"03_Merging/TriallelicSnps_2\"])\n",
    "#Flip the Euro dataset\n",
    "subprocess.run([\"plink\", \"--bfile\", \"02_Cleaning/Clean_Euro180_176ppl_317K_hg19_ATGC\", \"--flip\", \"03_Merging/TriallelicSnps_2.missnp\",\n",
    "                \"--make-bed\", \"--out\", \"02_Cleaning/Clean_Euro180_176ppl_317K_hg19_ATGC_flip\"])\n",
    "#Third merge\n",
    "subprocess.run([\"plink\", \"--merge-list\", \"ThirdMergeList.txt\", \"--out\", \"03_Merging/TriallelicSnps_3\"])\n",
    "\n",
    "#Get number of snps missing from first to third merge\n",
    "num_lines  = sum(1 for line in open(\"03_Merging/TriallelicSnps.missnp\"))\n",
    "num_lines2 = sum(1 for line in open(\"03_Merging/TriallelicSnps_2.missnp\"))\n",
    "num_lines3 = sum(1 for line in open(\"03_Merging/TriallelicSnps_3.missnp\"))\n",
    "print(num_lines, num_lines2, num_lines3)\n",
    "\n",
    "#removing snps from TriallelicSnps_3.missnp files from all datasets\n",
    "for file in glob.glob(\"02_Cleaning/*.bed\"):\n",
    "    inputname = file.split(\".\")\n",
    "    outname = \"CleanTriallelic_\" + inputname[0][18:]\n",
    "    print(\"Creating...\" + outname)\n",
    "    subprocess.run([\"plink\", \"--bfile\", inputname[0], \"--exclude\", \"03_Merging/TriallelicSnps_3.missnp\", \"--make-bed\", \"--out\", \"02_Cleaning/\" + outname])\n",
    "    \n",
    "print(\"Finished\")\n",
    "print(\"Final merging\")\n",
    "subprocess.run([\"plink\", \"--merge-list\", \"FinalMergeList.txt\", \"--out\", \"03_Merging/Merged\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will remove all snps with missing call rates greater than 0.1, SNPs with minor allele frequencies below 0.01, and a Hardy-Weinberg equilibrium exact test p-value below 1e-50.\n",
    "Finally, we will LD prune the set of SNPs using parameters 50, 5, and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run([\"plink\", \"--bfile\", \"03_Merging/Merged\", \"--geno\", \"0.1\", \"--maf\", \"0.01\", \"--hwe\", \"1e-50\", \"--make-bed\", \"--out\", \"04_CleanMerged/Cleaned\"])\n",
    "subprocess.run([\"plink\", \"--bfile\", \"04_CleanMerged/Cleaned\", \"--indep\", \"50\", \"5\", \"2\", \"--out\", \"04_CleanMerged/ExtractSNPs\"])\n",
    "with open(\"04_CleanMerged/ExtractSNPs.log\") as myfile:\n",
    "    for num, line in enumerate(myfile, 1):\n",
    "        if \"variants\" in line:\n",
    "            print(line, end='')\n",
    "    print(\"Finished file... \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365576 variants loaded from .bim file.\n",
      "--extract: 19498 variants remaining.\n",
      "19498 variants and 5290 people pass filters and QC.\n",
      "Finished file... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "subprocess.run([\"plink\", \"--bfile\", \"04_CleanMerged/Cleaned\", \"--extract\", \"04_CleanMerged/ExtractSNPs.prune.in\", \"--remove\", \"04_CleanMerged/ExtractSNPs.nosex\", \n",
    "                \"--make-bed\", \"--out\", \"04_CleanMerged/LDCleanMerged\"])\n",
    "with open(\"04_CleanMerged/CleanMerged.log\") as myfile:\n",
    "    for num, line in enumerate(myfile, 1):\n",
    "        if \"variants\" in line:\n",
    "            print(line, end='')\n",
    "    print(\"Finished file... \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll remove intermediary datasets to clear space, and move the final dataset to be latter merged with the reference samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove source files\n",
    "for f in glob.glob(\"02_Cleaning/*.*\"):\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in glob.glob(\"03_Merging/*.*\"):\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy final file to Merge1000G to be merged with the 1000Genomes samples\n",
    "dest_dir = os.path.join(projpath, 'DataBases', 'Genotypes', '05_ReferenceSamples')\n",
    "for filename in glob.glob(\"04_CleanMerged/CleanMerged.*\"):\n",
    "    shutil.copy(filename, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section delete the merging of the reference samples, and skip to the merging of the reference with our samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging reference and in-house samples\n",
    "\n",
    "Finally, we will merge the in-house samples with the reference samples from HGDP and 1000G.\n",
    "To do that we will extract the snps from the in-house samples already pruned and cleaned.\n",
    "Follow these [steps](https://tomszar.github.io/HGDP_1000G_Merge/) to merge the reference samples. \n",
    "Paste the final file `hgdp1000ghg19` in the 05_ReferenceSamples folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLINK v1.90b4 64-bit (20 Mar 2017)\n",
      "Options in effect:\n",
      "  --bfile CleanMerged_subset_flip\n",
      "  --bmerge hgdp1000ghg19_subset\n",
      "  --make-bed\n",
      "  --out HouseHGDP1000Ghg19\n",
      "\n",
      "Hostname: tomasgazelle\n",
      "Working directory: /home/tomas/Downloads/FacialSD/Results/MergeGeno/MergeSamples/05_ReferenceSamples\n",
      "Start time: Mon May 14 11:57:17 2018\n",
      "\n",
      "Random number seed: 1526313437\n",
      "3865 MB RAM detected; reserving 1932 MB for main workspace.\n",
      "5290 people loaded from CleanMerged_subset_flip.fam.\n",
      "3444 people to be merged from hgdp1000ghg19_subset.fam.\n",
      "Of these, 3444 are new, while 0 are present in the base dataset.\n",
      "12890 markers loaded from CleanMerged_subset_flip.bim.\n",
      "12890 markers to be merged from hgdp1000ghg19_subset.bim.\n",
      "Of these, 0 are new, while 12890 are present in the base dataset.\n",
      "Performing single-pass merge (8734 people, 12890 variants).\n",
      "Merged fileset written to HouseHGDP1000Ghg19-merge.bed +\n",
      "HouseHGDP1000Ghg19-merge.bim + HouseHGDP1000Ghg19-merge.fam .\n",
      "12890 variants loaded from .bim file.\n",
      "8734 people (3723 males, 5011 females) loaded from .fam.\n",
      "Using 1 thread (no multithreaded calculations invoked).\n",
      "Before main variant filters, 8611 founders and 123 nonfounders present.\n",
      "Calculating allele frequencies... done.\n",
      "Total genotyping rate is 0.9865.\n",
      "12890 variants and 8734 people pass filters and QC.\n",
      "Note: No phenotypes present.\n",
      "--make-bed to HouseHGDP1000Ghg19.bed + HouseHGDP1000Ghg19.bim +\n",
      "HouseHGDP1000Ghg19.fam ... done.\n",
      "\n",
      "End time: Mon May 14 11:57:18 2018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.join(outhouse, \"05_ReferenceSamples\"))\n",
    "subprocess.run([\"plink\", \"--bfile\", \"hgdp1000ghg19\", \"--extract\", \"CleanMerged.bim\", \"--make-bed\", \"--out\", \"hgdp1000ghg19_subset\"])\n",
    "subprocess.run([\"plink\", \"--bfile\", \"CleanMerged\", \"--extract\", \"hgdp1000ghg19_subset.bim\", \"--make-bed\", \"--out\", \"CleanMerged_subset\"])\n",
    "subprocess.run([\"plink\", \"--bfile\", \"CleanMerged_subset\", \"--bmerge\", \"hgdp1000ghg19_subset\", \"--make-bed\", \"--out\", \"HouseHGDP1000Ghg19\"])\n",
    "#Fliping strand and merging\n",
    "subprocess.run([\"plink\", \"--bfile\", \"CleanMerged_subset\", \"--flip\", \"HouseHGDP1000Ghg19-merge.missnp\", \"--make-bed\", \"--out\", \"CleanMerged_subset_flip\"])\n",
    "subprocess.run([\"plink\", \"--bfile\", \"CleanMerged_subset_flip\", \"--bmerge\", \"hgdp1000ghg19_subset\", \"--make-bed\", \"--out\", \"HouseHGDP1000Ghg19\"])\n",
    "\n",
    "with open(\"HouseHGDP1000Ghg19.log\", 'r') as fin:\n",
    "    file_contents = fin.read()\n",
    "    print(file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population stratification\n",
    "\n",
    "Now, we will load the final dataset created before and run some populations stratification analyses (`PCA`, `MDS` and `ADMIXTURE`)\n",
    "\n",
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(outhouse, \"05_ReferenceSamples\"))\n",
    "subprocess.run([\"plink\", \"--bfile\", \"HouseHGDP1000Ghg19\", \"--pca\", \"50\", \"--pca-cluster-names\", \"0\", \"--within\", \"hgdp1000ghg19.fam\", \"--out\", \"PCA\"])\n",
    "for file in glob.glob(\"PCA.*\"):\n",
    "    shutil.move(file, os.path.join(projpath, \"Results\", \"GenPCA\", file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
