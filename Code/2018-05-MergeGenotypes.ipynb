{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging genotypes\n",
    "\n",
    "This script will merge several genotypes files\n",
    "- Merging between participants' genotype platforms\n",
    "- Merging reference samples (1000G and HGDP)\n",
    "- Merging reference with participants' samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "First let's import modules and set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, shutil, subprocess, csv, time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tomas/Downloads/FacialSD/Results/MergeGeno/MergeSamples'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "projpath = os.path.realpath('..')\n",
    "outhouse = os.path.join(projpath, 'Results', 'MergeGeno', 'MergeSamples')\n",
    "outhgdp = os.path.join(projpath, 'Results', 'MergeGeno', 'temp', 'HGDP')\n",
    "out1000 = os.path.join(projpath, 'Results', 'MergeGeno', 'temp', '1000G')\n",
    "os.chdir(outhouse)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging in house genotypes\n",
    "\n",
    "The first step will be to merge our participants' genotypes. \n",
    "The steps will be as follows:\n",
    "- Clean each dataset by removing SNPs with missing rates greater than 0.1\n",
    "- Merge the datasets, and remove all problematic SNPs\n",
    "- Finally, LD prune SNPs in the merged dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will do a cleaning in each data set to remove SNP with missing call rates greater than 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating...Clean_UIUC2013_116ppl_959Ksnps_hg19_ATGC\n",
      "Creating...Clean_Euro180_176ppl_317K_hg19_ATGC\n",
      "Creating...Clean_CHP_1022ppl_114K_hg19_ATGC\n",
      "Creating...Clean_SA_231ppl_599K_hg19_ATGC\n",
      "Creating...Clean_TD_198ppl_1M_hg19_ATGC\n",
      "Creating...Clean_ADAPT_2784ppl_1Msnps_hg19_ATGC\n",
      "Creating...Clean_GHPAFF_3ppl_907K_hg19_ATGC\n",
      "Creating...Clean_CV_697ppl_964K_hg19_ATGC\n",
      "Creating...Clean_UIUC2014_168ppl_703K_hg19_ATGC\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#Clean datasets\n",
    "for file in glob.glob(\"01_Originals/*.bed\"):\n",
    "    inputname = file.split(\".\")\n",
    "    outname = \"Clean_\" + inputname[0][13:]\n",
    "    print(\"Creating...\" + outname)\n",
    "    subprocess.run([\"plink\", \"--bfile\", inputname[0], \"--geno\", \"--make-bed\", \"--out\", \"02_Cleaning/\" + outname])\n",
    "    \n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can take a look at the loaded and removed SNPs in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file: Clean_CHP_1022ppl_114K_hg19_ATGC\n",
      "114495 variants loaded from .bim file.\n",
      "1132 variants removed due to missing genotype data (--geno).\n",
      "113363 variants and 1022 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_UIUC2013_116ppl_959Ksnps_hg19_ATGC\n",
      "959382 variants loaded from .bim file.\n",
      "30151 variants removed due to missing genotype data (--geno).\n",
      "929231 variants and 116 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_GHPAFF_3ppl_907K_hg19_ATGC\n",
      "907494 variants loaded from .bim file.\n",
      "52823 variants removed due to missing genotype data (--geno).\n",
      "854671 variants and 3 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_TD_198ppl_1M_hg19_ATGC\n",
      "1032848 variants loaded from .bim file.\n",
      "454245 variants removed due to missing genotype data (--geno).\n",
      "578603 variants and 198 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_CV_697ppl_964K_hg19_ATGC\n",
      "964041 variants loaded from .bim file.\n",
      "0 variants removed due to missing genotype data (--geno).\n",
      "964041 variants and 697 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_UIUC2014_168ppl_703K_hg19_ATGC\n",
      "703725 variants loaded from .bim file.\n",
      "141590 variants removed due to missing genotype data (--geno).\n",
      "562135 variants and 168 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_Euro180_176ppl_317K_hg19_ATGC\n",
      "317503 variants loaded from .bim file.\n",
      "3416 variants removed due to missing genotype data (--geno).\n",
      "314087 variants and 176 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_SA_231ppl_599K_hg19_ATGC\n",
      "599855 variants loaded from .bim file.\n",
      "25249 variants removed due to missing genotype data (--geno).\n",
      "574606 variants and 231 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_ADAPT_2784ppl_1Msnps_hg19_ATGC\n",
      "1060559 variants loaded from .bim file.\n",
      "492757 variants removed due to missing genotype data (--geno).\n",
      "567802 variants and 2784 people pass filters and QC.\n",
      "Finished file... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in glob.glob(\"02_Cleaning/*.log\"):\n",
    "    with open(file) as myfile:\n",
    "        print(\"In file: \" + file.split(\".\")[0][12:])\n",
    "        for num, line in enumerate(myfile, 1):\n",
    "            if \"variants\" in line:\n",
    "                print(line, end='')\n",
    "        print(\"Finished file... \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will generate a first merging and we'll then extract the possibly triallelic snps from each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating...CleanTriallelic_TD_198ppl_1M_hg19_ATGC\n",
      "Creating...CleanTriallelic_GHPAFF_3ppl_907K_hg19_ATGC\n",
      "Creating...CleanTriallelic_Euro180_176ppl_317K_hg19_ATGC\n",
      "Creating...CleanTriallelic_UIUC2014_168ppl_703K_hg19_ATGC\n",
      "Creating...CleanTriallelic_CV_697ppl_964K_hg19_ATGC\n",
      "Creating...CleanTriallelic_SA_231ppl_599K_hg19_ATGC\n",
      "Creating...CleanTriallelic_ADAPT_2784ppl_1Msnps_hg19_ATGC\n",
      "Creating...CleanTriallelic_CHP_1022ppl_114K_hg19_ATGC\n",
      "Creating...CleanTriallelic_UIUC2013_116ppl_959Ksnps_hg19_ATGC\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "subprocess.run([\"plink\", \"--merge-list\", \"FirstMergeList.txt\", \"--out\", \"03_Merging/TriallelicSnps\"])\n",
    "for file in glob.glob(\"02_Cleaning/*.bed\"):\n",
    "    inputname = file.split(\".\")\n",
    "    outname = \"CleanTriallelic_\" + inputname[0][18:]\n",
    "    print(\"Creating...\" + outname)\n",
    "    subprocess.run([\"plink\", \"--bfile\", inputname[0], \"--exclude\", \"03_Merging/TriallelicSnps.missnp\", \"--make-bed\", \"--out\", \"02_Cleaning/\" + outname])\n",
    "    \n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Remember that there are still a lot of variants with different possitions across datasets.\n",
    "Now we will run a second merging, this time using the datasets with the excluded triallelic snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['plink', '--merge-list', 'FinalMergeList.txt', '--out', '03_Merging/Merged'], returncode=0)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"plink\", \"--merge-list\", \"FinalMergeList.txt\", \"--out\", \"03_Merging/Merged\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will remove all snps with missing call rates greater than 0.1, and LD prune the set of SNPs using parameters 50, 5, and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1278792 variants loaded from .bim file.\n",
      "1250083 variants removed due to missing genotype data (--geno).\n",
      "28709 variants and 5387 people pass filters and QC.\n",
      "Pruned 1005 variants from chromosome 1, leaving 1856.\n",
      "Pruned 633 variants from chromosome 2, leaving 1344.\n",
      "Pruned 612 variants from chromosome 3, leaving 1216.\n",
      "Pruned 257 variants from chromosome 4, leaving 602.\n",
      "Pruned 522 variants from chromosome 5, leaving 1156.\n",
      "Pruned 699 variants from chromosome 6, leaving 1147.\n",
      "Pruned 441 variants from chromosome 7, leaving 866.\n",
      "Pruned 640 variants from chromosome 8, leaving 1077.\n",
      "Pruned 416 variants from chromosome 9, leaving 762.\n",
      "Pruned 465 variants from chromosome 10, leaving 831.\n",
      "Pruned 738 variants from chromosome 11, leaving 1408.\n",
      "Pruned 378 variants from chromosome 12, leaving 818.\n",
      "Pruned 237 variants from chromosome 13, leaving 554.\n",
      "Pruned 625 variants from chromosome 14, leaving 981.\n",
      "Pruned 405 variants from chromosome 15, leaving 771.\n",
      "Pruned 281 variants from chromosome 16, leaving 555.\n",
      "Pruned 383 variants from chromosome 17, leaving 735.\n",
      "Pruned 114 variants from chromosome 18, leaving 231.\n",
      "Pruned 114 variants from chromosome 19, leaving 429.\n",
      "Pruned 286 variants from chromosome 20, leaving 572.\n",
      "Pruned 345 variants from chromosome 21, leaving 691.\n",
      "Pruned 160 variants from chromosome 22, leaving 342.\n",
      "Pruned 0 variants from chromosome 23, leaving 9.\n",
      "Pruning complete.  9756 of 28709 variants removed.\n",
      "Finished file... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "subprocess.run([\"plink\", \"--bfile\", \"03_Merging/Merged\", \"--geno\", \"--indep\", \"50\", \"5\", \"2\", \"--out\", \"04_CleanMerged/ExtractSNPs\"])\n",
    "with open(\"04_CleanMerged/ExtractSNPs.log\") as myfile:\n",
    "    for num, line in enumerate(myfile, 1):\n",
    "        if \"variants\" in line:\n",
    "            print(line, end='')\n",
    "    print(\"Finished file... \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1278792 variants loaded from .bim file.\n",
      "--extract: 18953 variants remaining.\n",
      "18953 variants and 5290 people pass filters and QC.\n",
      "Finished file... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "subprocess.run([\"plink\", \"--bfile\", \"03_Merging/Merged\", \"--extract\", \"04_CleanMerged/ExtractSNPs.prune.in\", \"--remove\", \"04_CleanMerged/ExtractSNPs.nosex\", \n",
    "                \"--make-bed\", \"--out\", \"04_CleanMerged/CleanMerged\"])\n",
    "with open(\"04_CleanMerged/CleanMerged.log\") as myfile:\n",
    "    for num, line in enumerate(myfile, 1):\n",
    "        if \"variants\" in line:\n",
    "            print(line, end='')\n",
    "    print(\"Finished file... \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll remove intermediary datasets to clear space, and move the final dataset to be latter merged with the reference samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove source files\n",
    "for f in glob.glob(\"02_Cleaning/*.*\"):\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in glob.glob(\"03_Merging/*.*\"):\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy final file to Merge1000G to be merged with the 1000Genomes samples\n",
    "dest_dir = os.path.join(projpath, 'Results', 'MergeGeno', 'MergeSamples', '05_ReferenceSamples')\n",
    "for filename in glob.glob(\"04_CleanMerged/CleanMerged.*\"):\n",
    "    shutil.copy(filename, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging reference samples\n",
    "\n",
    "In the second step, we will merge the reference samples from 1000G and HGDP.\n",
    "We will follow these steps:\n",
    "- Download HGDP files, and transform them into a plink file\n",
    "- Download 1000G files and keep only SNPs found in the HGDP files\n",
    "- Merge the HGDP and 1000G files\n",
    "\n",
    "### HGDP\n",
    "\n",
    "The HGDP files (Stanford) were downloaded from [here](http://hagsc.org/hgdp/files.html), and the sample list file from [here](http://www.stanford.edu/group/rosenberglab/data/rosenberg2006ahg/SampleInformation.txt).\n",
    "The script to transform the HGDP data to plink format is called HGDPtoPlink.sh and was modified from [here](http://www.harappadna.org/2011/02/hgdp-to-ped-conversion/).\n",
    "The HGDP data uses coordinates from build 36.1 (a list of assemblies can be found [here](https://genome.ucsc.edu/FAQ/FAQreleases.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['bash', 'HGDPtoPlink.sh'], returncode=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run the HGDPtoPlink script\n",
    "os.chdir(os.path.join(projpath, 'Code'))\n",
    "subprocess.run([\"bash\", \"HGDPtoPlink.sh\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the 1000G uses the GRCh37 assembly (fasta file can be found [here](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/) as human_g1k_v37.fasta.gz) we'll need to liftover the HGDP coordinates.\n",
    "To do that we'll use UCSC [liftOver](http://genome.ucsc.edu/cgi-bin/hgLiftOver) already installed using bioconda, and [liftOverPlink](https://github.com/sritchie73/liftOverPlink) as a wrapper to work with plink files (`ped` and `map` formats).\n",
    "The chain file that tells liftOver how to convert between hg18 and hg19 can be downloaded [here](http://hgdownload.cse.ucsc.edu/goldenPath/hg18/liftOver/hg18ToHg19.over.chain.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting MAP file to UCSC BED file...\n",
      "SUCC:  map->bed succ\n",
      "Lifting BED file...\n",
      "SUCC:  liftBed succ\n",
      "Converting lifted BED file back to MAP...\n",
      "SUCC:  bed->map succ\n",
      "cleaning up BED files...\n"
     ]
    }
   ],
   "source": [
    "os.chdir(outhgdp)\n",
    "#Using liftover\n",
    "%run liftOverPlink.py --map hgdp940.map --out lifted --chain hg18ToHg19.over.chain.gz\n",
    "%run rmBadLifts.py --map lifted.map --out good_lifted.map --log bad_lifted.dat\n",
    "#Creating a list of snps to include in lifted version\n",
    "snps = pd.read_csv(\"good_lifted.map\", sep = \"\\t\", header = None)\n",
    "snps.iloc[:,1].to_csv(\"snplist.txt\", index = False)\n",
    "#Excluding snps and creating binary file\n",
    "subprocess.run([\"plink\", \"--file\", \"hgdp940\", \"--recode\", \"--out\", \"lifted\", \"--extract\", \"snplist.txt\" ])\n",
    "subprocess.run([\"plink\", \"--file\", \"--ped\", \"lifted.ped\", \"--map\", \"good_lifted.map\", \"--make-bed\", \"--out\", \"hgdp940hg19\"])\n",
    "\n",
    "#Removing some files\n",
    "for file in glob.glob(\"*.ped\"):\n",
    "    os.remove(file)\n",
    "    \n",
    "for file in glob.glob(\"*.map\"):\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing single-pass .bed write (644054 variants, 940 people).\n",
      "644054 variants loaded from .bim file.\n",
      "644054 variants and 940 people pass filters and QC.\n",
      "Finished file... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read hgdp940hg19 log file\n",
    "with open(\"hgdp940hg19.log\") as myfile:\n",
    "    for num, line in enumerate(myfile, 1):\n",
    "        if \"variants\" in line:\n",
    "            print(line, end='')\n",
    "    print(\"Finished file... \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = os.path.join(projpath, 'Results', 'MergeGeno', 'temp', 'Merge')\n",
    "for filename in glob.glob(\"hgdp940hg19.*\"):\n",
    "    shutil.copy(filename, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1000G\n",
    "\n",
    "The 1000G Phase 3 files were downloaded from [here](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/).\n",
    "First, we will extract the list of snps (`snplist.txt`) from the HGDP dataset for each chromosome of the 1000G samples using vcftools.\n",
    "Then we will concatenate the different autosomal chromosomes in one file and convert it into a plink binary file using bcftools and plink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(out1000)\n",
    "shutil.copy(os.path.join(outhgdp, \"snplist.txt\"), out1000)\n",
    "for file in glob.glob(\"*chr[0-9]*.gz\"):\n",
    "    outname = file.split(\".\")[1] + \"_extracted\"\n",
    "    subprocess.run([\"vcftools\", \"--gzvcf\", file, \"--snps\", \"snplist.txt\", \"--recode\", \"--out\", outname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['bcftools', 'concat', '-o', '1000g.vcf.gz', '-Oz', 'chr21_extracted.recode.vcf', 'chr13_extracted.recode.vcf', 'chr4_extracted.recode.vcf', 'chr19_extracted.recode.vcf', 'chr8_extracted.recode.vcf', 'chr18_extracted.recode.vcf', 'chr14_extracted.recode.vcf', 'chr3_extracted.recode.vcf', 'chr11_extracted.recode.vcf', 'chr7_extracted.recode.vcf', 'chr16_extracted.recode.vcf', 'chr10_extracted.recode.vcf', 'chr17_extracted.recode.vcf', 'chr5_extracted.recode.vcf', 'chr2_extracted.recode.vcf', 'chr6_extracted.recode.vcf', 'chr9_extracted.recode.vcf', 'chr1_extracted.recode.vcf', 'chr15_extracted.recode.vcf', 'chr12_extracted.recode.vcf', 'chr22_extracted.recode.vcf', 'chr20_extracted.recode.vcf'], returncode=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatfiles = glob.glob(\"chr[0-9]*.recode.vcf\")\n",
    "function = [\"bcftools\", \"concat\", \"-o\", \"1000g.vcf.gz\", \"-Oz\"]\n",
    "function.extend(concatfiles)\n",
    "subprocess.run(function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclude related people and convert to binary plink\n",
    "subprocess.run([\"plink\", \"--vcf\", \"1000g.vcf.gz\", \"--make-bed\", \"--out\", \"1000Ghg19\" ])\n",
    "#Updating fam file\n",
    "allfam = pd.read_csv(\"integrated_call_samples_v2.20130502.ALL.ped\", header = None, skiprows = 1, sep = \"\\t\")\n",
    "oldfam = pd.read_csv(\"1000Ghg19.fam\", header = None, sep = \" \")\n",
    "updatedfam = pd.merge(oldfam, allfam, how = \"inner\", left_on = 1, right_on = 1)\n",
    "updatedfam.iloc[:,[6,1,7,8,9,5]].to_csv(\"1000Ghg19.fam\", sep = \" \", header = False, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob(\"chr*.recode.vcf\"):\n",
    "    os.remove(file)\n",
    "    \n",
    "os.remove(\"1000g.vcf.gz\")\n",
    "\n",
    "dest_dir = os.path.join(projpath, 'Results', 'MergeGeno', 'temp', 'Merge')\n",
    "for filename in glob.glob(\"1000Ghg19.*\"):\n",
    "    shutil.copy(filename, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge reference samples\n",
    "\n",
    "Now we will merge the 1000G and HGDP databases, both using the hg19 coordinates and with related people removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(projpath, 'Results', 'MergeGeno', 'temp', 'Merge'))\n",
    "subprocess.run([\"plink\", \"--bfile\", \"1000Ghg19\", \"--bmerge\", \"hgdp940hg19\", \"--make-bed\", \"--out\", \"hgdp1000ghg19\"])\n",
    "for file in glob.glob(\"*.bed\"):\n",
    "    outname = file.split(\".\")[0] + \"_temp\"\n",
    "    subprocess.run([\"plink\", \"--bfile\", file.split(\".\")[0], \"--exclude\", \"hgdp1000ghg19-merge.missnp\", \"--make-bed\", \"--out\", outname])\n",
    "\n",
    "subprocess.run([\"plink\", \"--bfile\", \"hgdp940hg19_temp\", \"--bmerge\", \"1000Ghg19_temp\", \"--make-bed\", \"--out\", \"hgdp1000ghg19\"])\n",
    "for file in glob.glob(\"*_temp*\"):\n",
    "    os.remove(file)\n",
    "    \n",
    "dest_dir = os.path.join(projpath, 'Results', 'MergeGeno', 'MergeSamples', '05_ReferenceSamples')\n",
    "for filename in glob.glob(\"hgdp1000ghg19.*\"):\n",
    "    shutil.copy(filename, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging reference and in-house samples\n",
    "\n",
    "Finally, we will merge the in-house samples with the reference samples from HGDP and 1000G. \n",
    "To do that we will extract the snps from the in-house samples already pruned and cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLINK v1.90b4 64-bit (20 Mar 2017)\n",
      "Options in effect:\n",
      "  --bfile CleanMerged_subset_flip\n",
      "  --bmerge hgdp1000ghg19_subset\n",
      "  --make-bed\n",
      "  --out HouseHGDP1000Ghg19\n",
      "\n",
      "Hostname: tomasgazelle\n",
      "Working directory: /home/tomas/Downloads/FacialSD/Results/MergeGeno/MergeSamples/05_ReferenceSamples\n",
      "Start time: Mon May 14 11:57:17 2018\n",
      "\n",
      "Random number seed: 1526313437\n",
      "3865 MB RAM detected; reserving 1932 MB for main workspace.\n",
      "5290 people loaded from CleanMerged_subset_flip.fam.\n",
      "3444 people to be merged from hgdp1000ghg19_subset.fam.\n",
      "Of these, 3444 are new, while 0 are present in the base dataset.\n",
      "12890 markers loaded from CleanMerged_subset_flip.bim.\n",
      "12890 markers to be merged from hgdp1000ghg19_subset.bim.\n",
      "Of these, 0 are new, while 12890 are present in the base dataset.\n",
      "Performing single-pass merge (8734 people, 12890 variants).\n",
      "Merged fileset written to HouseHGDP1000Ghg19-merge.bed +\n",
      "HouseHGDP1000Ghg19-merge.bim + HouseHGDP1000Ghg19-merge.fam .\n",
      "12890 variants loaded from .bim file.\n",
      "8734 people (3723 males, 5011 females) loaded from .fam.\n",
      "Using 1 thread (no multithreaded calculations invoked).\n",
      "Before main variant filters, 8611 founders and 123 nonfounders present.\n",
      "Calculating allele frequencies... done.\n",
      "Total genotyping rate is 0.9865.\n",
      "12890 variants and 8734 people pass filters and QC.\n",
      "Note: No phenotypes present.\n",
      "--make-bed to HouseHGDP1000Ghg19.bed + HouseHGDP1000Ghg19.bim +\n",
      "HouseHGDP1000Ghg19.fam ... done.\n",
      "\n",
      "End time: Mon May 14 11:57:18 2018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.join(outhouse, \"05_ReferenceSamples\"))\n",
    "subprocess.run([\"plink\", \"--bfile\", \"hgdp1000ghg19\", \"--extract\", \"CleanMerged.bim\", \"--make-bed\", \"--out\", \"hgdp1000ghg19_subset\"])\n",
    "subprocess.run([\"plink\", \"--bfile\", \"CleanMerged\", \"--extract\", \"hgdp1000ghg19_subset.bim\", \"--make-bed\", \"--out\", \"CleanMerged_subset\"])\n",
    "subprocess.run([\"plink\", \"--bfile\", \"CleanMerged_subset\", \"--bmerge\", \"hgdp1000ghg19_subset\", \"--make-bed\", \"--out\", \"HouseHGDP1000Ghg19\"])\n",
    "#Fliping strand and merging\n",
    "subprocess.run([\"plink\", \"--bfile\", \"CleanMerged_subset\", \"--flip\", \"HouseHGDP1000Ghg19-merge.missnp\", \"--make-bed\", \"--out\", \"CleanMerged_subset_flip\"])\n",
    "subprocess.run([\"plink\", \"--bfile\", \"CleanMerged_subset_flip\", \"--bmerge\", \"hgdp1000ghg19_subset\", \"--make-bed\", \"--out\", \"HouseHGDP1000Ghg19\"])\n",
    "\n",
    "with open(\"HouseHGDP1000Ghg19.log\", 'r') as fin:\n",
    "    file_contents = fin.read()\n",
    "    print(file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population stratification\n",
    "\n",
    "Now, we will load the final dataset created before and run some populations stratification analyses (`PCA`, `MDS` and `ADMIXTURE`)\n",
    "\n",
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(outhouse, \"05_ReferenceSamples\"))\n",
    "subprocess.run([\"plink\", \"--bfile\", \"HouseHGDP1000Ghg19\", \"--pca\", \"50\", \"--pca-cluster-names\", \"0\", \"--within\", \"hgdp1000ghg19.fam\", \"--out\", \"PCA\"])\n",
    "for file in glob.glob(\"PCA.*\"):\n",
    "    shutil.move(file, os.path.join(projpath, \"Results\", \"GenPCA\", file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Admixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergesamples = os.path.join(outhouse, \"05_ReferenceSamples\", \"HouseHGDP1000Ghg19.bed\")\n",
    "os.chdir(os.path.join(projpath, \"Results\", \"Admixture\"))\n",
    "for i in range(2,11):\n",
    "    f = open(\"log_\" + str(i) + \".txt\", \"w\")\n",
    "    subprocess.run([\"./admixture\", \"--cv\", mergesamples, str(i)], stdout=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
