{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging genotypes\n",
    "\n",
    "This script will merge several genotypes files\n",
    "- Merging between participants' genotype platforms\n",
    "- Merging reference samples with participants' samples\n",
    "\n",
    "For reference samples we will use the 1000G and HGDP samples. \n",
    "To see the steps to merge the reference samples see [here](https://tomszar.github.io/HGDP_1000G_Merge/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "First let's import modules and set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, shutil, subprocess, csv, time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "projpath  = os.path.realpath('..')\n",
    "pathgenos = os.path.join(projpath, \"DataBases\", \"Genotypes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging in house genotypes\n",
    "\n",
    "The first step will be to merge our participants' genotypes. \n",
    "The steps will be as follows:\n",
    "- Clean each dataset by removing SNPs with missing rates greater than 0.1\n",
    "- Merge the datasets, and remove all problematic SNPs\n",
    "- Finally, LD prune SNPs in the merged dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will do a cleaning in each data set to remove SNP with missing call rates greater than 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating...Clean_UIUC2013_116ppl_959Ksnps_hg19_ATGC\n",
      "Creating...Clean_Euro180_176ppl_317K_hg19_ATGC\n",
      "Creating...Clean_CHP_1022ppl_114K_hg19_ATGC\n",
      "Creating...Clean_SA_231ppl_599K_hg19_ATGC\n",
      "Creating...Clean_TD_198ppl_1M_hg19_ATGC\n",
      "Creating...Clean_ADAPT_2784ppl_1Msnps_hg19_ATGC\n",
      "Creating...Clean_GHPAFF_3ppl_907K_hg19_ATGC\n",
      "Creating...Clean_CV_697ppl_964K_hg19_ATGC\n",
      "Creating...Clean_UIUC2014_168ppl_703K_hg19_ATGC\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#Clean datasets\n",
    "os.chdir(pathgenos)\n",
    "for file in glob.glob(\"01_Originals/*.bed\"):\n",
    "    inputname = file.split(\".\")\n",
    "    outname = \"Clean_\" + inputname[0][13:]\n",
    "    print(\"Creating...\" + outname)\n",
    "    subprocess.run([\"plink\", \"--bfile\", inputname[0], \"--geno\", \"--make-bed\", \"--out\", \"02_Cleaning/\" + outname])\n",
    "    \n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can take a look at the loaded and removed SNPs in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file: Clean_CHP_1022ppl_114K_hg19_ATGC\n",
      "114495 variants loaded from .bim file.\n",
      "1132 variants removed due to missing genotype data (--geno).\n",
      "113363 variants and 1022 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_UIUC2013_116ppl_959Ksnps_hg19_ATGC\n",
      "959382 variants loaded from .bim file.\n",
      "30151 variants removed due to missing genotype data (--geno).\n",
      "929231 variants and 116 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_GHPAFF_3ppl_907K_hg19_ATGC\n",
      "907494 variants loaded from .bim file.\n",
      "52823 variants removed due to missing genotype data (--geno).\n",
      "854671 variants and 3 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_TD_198ppl_1M_hg19_ATGC\n",
      "1032848 variants loaded from .bim file.\n",
      "454245 variants removed due to missing genotype data (--geno).\n",
      "578603 variants and 198 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_CV_697ppl_964K_hg19_ATGC\n",
      "964041 variants loaded from .bim file.\n",
      "0 variants removed due to missing genotype data (--geno).\n",
      "964041 variants and 697 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_UIUC2014_168ppl_703K_hg19_ATGC\n",
      "703725 variants loaded from .bim file.\n",
      "141590 variants removed due to missing genotype data (--geno).\n",
      "562135 variants and 168 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_Euro180_176ppl_317K_hg19_ATGC\n",
      "317503 variants loaded from .bim file.\n",
      "3416 variants removed due to missing genotype data (--geno).\n",
      "314087 variants and 176 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_SA_231ppl_599K_hg19_ATGC\n",
      "599855 variants loaded from .bim file.\n",
      "25249 variants removed due to missing genotype data (--geno).\n",
      "574606 variants and 231 people pass filters and QC.\n",
      "Finished file... \n",
      "\n",
      "In file: Clean_ADAPT_2784ppl_1Msnps_hg19_ATGC\n",
      "1060559 variants loaded from .bim file.\n",
      "492757 variants removed due to missing genotype data (--geno).\n",
      "567802 variants and 2784 people pass filters and QC.\n",
      "Finished file... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in glob.glob(\"02_Cleaning/*.log\"):\n",
    "    with open(file) as myfile:\n",
    "        print(\"In file: \" + file.split(\".\")[0][12:])\n",
    "        for num, line in enumerate(myfile, 1):\n",
    "            if \"variants\" in line:\n",
    "                print(line, end='')\n",
    "        print(\"Finished file... \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will generate a first merging to get a list of problematic snps.\n",
    "Based on comparing a few snps across datasets, it seems that the CV dataset contains most fliped snps, followed by the Euro dataset.\n",
    "Then, we will flip the snps of the CV dataset, merge, flip the Euro dataset, and merge again.\n",
    "Finally, we'll then extract the possibly triallelic snps from each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90622 16328 3838\n",
      "Creating...CleanTriallelic_TD_198ppl_1M_hg19_ATGC\n",
      "Creating...CleanTriallelic_GHPAFF_3ppl_907K_hg19_ATGC\n",
      "Creating...CleanTriallelic_Euro180_176ppl_317K_hg19_ATGC\n",
      "Creating...CleanTriallelic_UIUC2014_168ppl_703K_hg19_ATGC\n",
      "Creating...CleanTriallelic_CV_697ppl_964K_hg19_ATGC\n",
      "Creating...CleanTriallelic_Euro180_176ppl_317K_hg19_ATGC_flip\n",
      "Creating...CleanTriallelic_SA_231ppl_599K_hg19_ATGC\n",
      "Creating...CleanTriallelic_ADAPT_2784ppl_1Msnps_hg19_ATGC\n",
      "Creating...CleanTriallelic_CHP_1022ppl_114K_hg19_ATGC\n",
      "Creating...CleanTriallelic_CV_697ppl_964K_hg19_ATGC_flip\n",
      "Creating...CleanTriallelic_UIUC2013_116ppl_959Ksnps_hg19_ATGC\n",
      "Finished\n",
      "Final merging\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['plink', '--merge-list', 'FinalMergeList.txt', '--out', '03_Merging/Merged'], returncode=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First merge\n",
    "subprocess.run([\"plink\", \"--merge-list\", \"FirstMergeList.txt\", \"--out\", \"03_Merging/TriallelicSnps\"])\n",
    "#Flip the CV dataset\n",
    "subprocess.run([\"plink\", \"--bfile\", \"02_Cleaning/Clean_CV_697ppl_964K_hg19_ATGC\", \"--flip\", \"03_Merging/TriallelicSnps.missnp\",\n",
    "                \"--make-bed\", \"--out\", \"02_Cleaning/Clean_CV_697ppl_964K_hg19_ATGC_flip\"])\n",
    "#Second merge\n",
    "subprocess.run([\"plink\", \"--merge-list\", \"SecondMergeList.txt\", \"--out\", \"03_Merging/TriallelicSnps_2\"])\n",
    "#Flip the Euro dataset\n",
    "subprocess.run([\"plink\", \"--bfile\", \"02_Cleaning/Clean_Euro180_176ppl_317K_hg19_ATGC\", \"--flip\", \"03_Merging/TriallelicSnps_2.missnp\",\n",
    "                \"--make-bed\", \"--out\", \"02_Cleaning/Clean_Euro180_176ppl_317K_hg19_ATGC_flip\"])\n",
    "#Third merge\n",
    "subprocess.run([\"plink\", \"--merge-list\", \"ThirdMergeList.txt\", \"--out\", \"03_Merging/TriallelicSnps_3\"])\n",
    "\n",
    "#Get number of snps missing from first to third merge\n",
    "num_lines  = sum(1 for line in open(\"03_Merging/TriallelicSnps.missnp\"))\n",
    "num_lines2 = sum(1 for line in open(\"03_Merging/TriallelicSnps_2.missnp\"))\n",
    "num_lines3 = sum(1 for line in open(\"03_Merging/TriallelicSnps_3.missnp\"))\n",
    "print(num_lines, num_lines2, num_lines3)\n",
    "\n",
    "#removing snps from TriallelicSnps_3.missnp files from all datasets\n",
    "for file in glob.glob(\"02_Cleaning/*.bed\"):\n",
    "    inputname = file.split(\".\")\n",
    "    outname = \"CleanTriallelic_\" + inputname[0][18:]\n",
    "    print(\"Creating...\" + outname)\n",
    "    subprocess.run([\"plink\", \"--bfile\", inputname[0], \"--exclude\", \"03_Merging/TriallelicSnps_3.missnp\", \"--make-bed\", \"--out\", \"02_Cleaning/\" + outname])\n",
    "    \n",
    "print(\"Finished\")\n",
    "print(\"Final merging\")\n",
    "subprocess.run([\"plink\", \"--merge-list\", \"FinalMergeList.txt\", \"--out\", \"03_Merging/Merged\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will remove all snps with missing call rates greater than 0.1, and LD prune the set of SNPs using parameters 50, 5, and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365576 variants loaded from .bim file.\n",
      "1335704 variants removed due to missing genotype data (--geno).\n",
      "29872 variants and 5387 people pass filters and QC.\n",
      "Pruned 1077 variants from chromosome 1, leaving 1914.\n",
      "Pruned 675 variants from chromosome 2, leaving 1376.\n",
      "Pruned 652 variants from chromosome 3, leaving 1243.\n",
      "Pruned 265 variants from chromosome 4, leaving 621.\n",
      "Pruned 554 variants from chromosome 5, leaving 1201.\n",
      "Pruned 739 variants from chromosome 6, leaving 1170.\n",
      "Pruned 476 variants from chromosome 7, leaving 884.\n",
      "Pruned 669 variants from chromosome 8, leaving 1111.\n",
      "Pruned 436 variants from chromosome 9, leaving 784.\n",
      "Pruned 491 variants from chromosome 10, leaving 848.\n",
      "Pruned 798 variants from chromosome 11, leaving 1444.\n",
      "Pruned 409 variants from chromosome 12, leaving 849.\n",
      "Pruned 258 variants from chromosome 13, leaving 566.\n",
      "Pruned 658 variants from chromosome 14, leaving 1013.\n",
      "Pruned 439 variants from chromosome 15, leaving 784.\n",
      "Pruned 301 variants from chromosome 16, leaving 568.\n",
      "Pruned 406 variants from chromosome 17, leaving 759.\n",
      "Pruned 118 variants from chromosome 18, leaving 241.\n",
      "Pruned 123 variants from chromosome 19, leaving 446.\n",
      "Pruned 301 variants from chromosome 20, leaving 593.\n",
      "Pruned 357 variants from chromosome 21, leaving 717.\n",
      "Pruned 172 variants from chromosome 22, leaving 349.\n",
      "Pruned 0 variants from chromosome 23, leaving 17.\n",
      "Pruning complete.  10374 of 29872 variants removed.\n",
      "Finished file... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "subprocess.run([\"plink\", \"--bfile\", \"03_Merging/Merged\", \"--geno\", \"--indep\", \"50\", \"5\", \"2\", \"--out\", \"04_CleanMerged/ExtractSNPs\"])\n",
    "with open(\"04_CleanMerged/ExtractSNPs.log\") as myfile:\n",
    "    for num, line in enumerate(myfile, 1):\n",
    "        if \"variants\" in line:\n",
    "            print(line, end='')\n",
    "    print(\"Finished file... \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365576 variants loaded from .bim file.\n",
      "--extract: 19498 variants remaining.\n",
      "19498 variants and 5290 people pass filters and QC.\n",
      "Finished file... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "subprocess.run([\"plink\", \"--bfile\", \"03_Merging/Merged\", \"--extract\", \"04_CleanMerged/ExtractSNPs.prune.in\", \"--remove\", \"04_CleanMerged/ExtractSNPs.nosex\", \n",
    "                \"--make-bed\", \"--out\", \"04_CleanMerged/CleanMerged\"])\n",
    "with open(\"04_CleanMerged/CleanMerged.log\") as myfile:\n",
    "    for num, line in enumerate(myfile, 1):\n",
    "        if \"variants\" in line:\n",
    "            print(line, end='')\n",
    "    print(\"Finished file... \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll remove intermediary datasets to clear space, and move the final dataset to be latter merged with the reference samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove source files\n",
    "for f in glob.glob(\"02_Cleaning/*.*\"):\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in glob.glob(\"03_Merging/*.*\"):\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy final file to Merge1000G to be merged with the 1000Genomes samples\n",
    "dest_dir = os.path.join(projpath, 'DataBases', 'Genotypes', '05_ReferenceSamples')\n",
    "for filename in glob.glob(\"04_CleanMerged/CleanMerged.*\"):\n",
    "    shutil.copy(filename, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section delete the merging of the reference samples, and skip to the merging of the reference with our samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging reference samples\n",
    "\n",
    "In the second step, we will merge the reference samples from 1000G and HGDP.\n",
    "We will follow these steps:\n",
    "- Download HGDP files, and transform them into a plink file\n",
    "- Download 1000G files and keep only SNPs found in the HGDP files\n",
    "- Merge the HGDP and 1000G files\n",
    "\n",
    "### HGDP\n",
    "\n",
    "The HGDP files (Stanford) were downloaded from [here](http://hagsc.org/hgdp/files.html), and the sample list file from [here](http://www.stanford.edu/group/rosenberglab/data/rosenberg2006ahg/SampleInformation.txt).\n",
    "The script to transform the HGDP data to plink format is called HGDPtoPlink.sh and was modified from [here](http://www.harappadna.org/2011/02/hgdp-to-ped-conversion/).\n",
    "The HGDP data uses coordinates from build 36.1 (a list of assemblies can be found [here](https://genome.ucsc.edu/FAQ/FAQreleases.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['bash', 'HGDPtoPlink.sh'], returncode=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run the HGDPtoPlink script\n",
    "os.chdir(os.path.join(projpath, 'Code'))\n",
    "subprocess.run([\"bash\", \"HGDPtoPlink.sh\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the 1000G uses the GRCh37 assembly (fasta file can be found [here](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/) as human_g1k_v37.fasta.gz) we'll need to liftover the HGDP coordinates.\n",
    "To do that we'll use UCSC [liftOver](http://genome.ucsc.edu/cgi-bin/hgLiftOver) already installed using bioconda, and [liftOverPlink](https://github.com/sritchie73/liftOverPlink) as a wrapper to work with plink files (`ped` and `map` formats).\n",
    "The chain file that tells liftOver how to convert between hg18 and hg19 can be downloaded [here](http://hgdownload.cse.ucsc.edu/goldenPath/hg18/liftOver/hg18ToHg19.over.chain.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting MAP file to UCSC BED file...\n",
      "SUCC:  map->bed succ\n",
      "Lifting BED file...\n",
      "SUCC:  liftBed succ\n",
      "Converting lifted BED file back to MAP...\n",
      "SUCC:  bed->map succ\n",
      "cleaning up BED files...\n"
     ]
    }
   ],
   "source": [
    "os.chdir(outhgdp)\n",
    "#Using liftover\n",
    "%run liftOverPlink.py --map hgdp940.map --out lifted --chain hg18ToHg19.over.chain.gz\n",
    "%run rmBadLifts.py --map lifted.map --out good_lifted.map --log bad_lifted.dat\n",
    "#Creating a list of snps to include in lifted version\n",
    "snps = pd.read_csv(\"good_lifted.map\", sep = \"\\t\", header = None)\n",
    "snps.iloc[:,1].to_csv(\"snplist.txt\", index = False)\n",
    "#Excluding snps and creating binary file\n",
    "subprocess.run([\"plink\", \"--file\", \"hgdp940\", \"--recode\", \"--out\", \"lifted\", \"--extract\", \"snplist.txt\" ])\n",
    "subprocess.run([\"plink\", \"--file\", \"--ped\", \"lifted.ped\", \"--map\", \"good_lifted.map\", \"--make-bed\", \"--out\", \"hgdp940hg19\"])\n",
    "\n",
    "#Removing some files\n",
    "for file in glob.glob(\"*.ped\"):\n",
    "    os.remove(file)\n",
    "    \n",
    "for file in glob.glob(\"*.map\"):\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing single-pass .bed write (644054 variants, 940 people).\n",
      "644054 variants loaded from .bim file.\n",
      "644054 variants and 940 people pass filters and QC.\n",
      "Finished file... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read hgdp940hg19 log file\n",
    "with open(\"hgdp940hg19.log\") as myfile:\n",
    "    for num, line in enumerate(myfile, 1):\n",
    "        if \"variants\" in line:\n",
    "            print(line, end='')\n",
    "    print(\"Finished file... \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = os.path.join(projpath, 'Results', 'MergeGeno', 'temp', 'Merge')\n",
    "for filename in glob.glob(\"hgdp940hg19.*\"):\n",
    "    shutil.copy(filename, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1000G\n",
    "\n",
    "The 1000G Phase 3 files were downloaded from [here](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/).\n",
    "First, we will extract the list of snps (`snplist.txt`) from the HGDP dataset for each chromosome of the 1000G samples using vcftools.\n",
    "Then we will concatenate the different autosomal chromosomes in one file and convert it into a plink binary file using bcftools and plink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(out1000)\n",
    "shutil.copy(os.path.join(outhgdp, \"snplist.txt\"), out1000)\n",
    "for file in glob.glob(\"*chr[0-9]*.gz\"):\n",
    "    outname = file.split(\".\")[1] + \"_extracted\"\n",
    "    subprocess.run([\"vcftools\", \"--gzvcf\", file, \"--snps\", \"snplist.txt\", \"--recode\", \"--out\", outname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['bcftools', 'concat', '-o', '1000g.vcf.gz', '-Oz', 'chr21_extracted.recode.vcf', 'chr13_extracted.recode.vcf', 'chr4_extracted.recode.vcf', 'chr19_extracted.recode.vcf', 'chr8_extracted.recode.vcf', 'chr18_extracted.recode.vcf', 'chr14_extracted.recode.vcf', 'chr3_extracted.recode.vcf', 'chr11_extracted.recode.vcf', 'chr7_extracted.recode.vcf', 'chr16_extracted.recode.vcf', 'chr10_extracted.recode.vcf', 'chr17_extracted.recode.vcf', 'chr5_extracted.recode.vcf', 'chr2_extracted.recode.vcf', 'chr6_extracted.recode.vcf', 'chr9_extracted.recode.vcf', 'chr1_extracted.recode.vcf', 'chr15_extracted.recode.vcf', 'chr12_extracted.recode.vcf', 'chr22_extracted.recode.vcf', 'chr20_extracted.recode.vcf'], returncode=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatfiles = glob.glob(\"chr[0-9]*.recode.vcf\")\n",
    "function = [\"bcftools\", \"concat\", \"-o\", \"1000g.vcf.gz\", \"-Oz\"]\n",
    "function.extend(concatfiles)\n",
    "subprocess.run(function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclude related people and convert to binary plink\n",
    "subprocess.run([\"plink\", \"--vcf\", \"1000g.vcf.gz\", \"--make-bed\", \"--out\", \"1000Ghg19\" ])\n",
    "#Updating fam file\n",
    "allfam = pd.read_csv(\"integrated_call_samples_v2.20130502.ALL.ped\", header = None, skiprows = 1, sep = \"\\t\")\n",
    "oldfam = pd.read_csv(\"1000Ghg19.fam\", header = None, sep = \" \")\n",
    "updatedfam = pd.merge(oldfam, allfam, how = \"inner\", left_on = 1, right_on = 1)\n",
    "updatedfam.iloc[:,[6,1,7,8,9,5]].to_csv(\"1000Ghg19.fam\", sep = \" \", header = False, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob(\"chr*.recode.vcf\"):\n",
    "    os.remove(file)\n",
    "    \n",
    "os.remove(\"1000g.vcf.gz\")\n",
    "\n",
    "dest_dir = os.path.join(projpath, 'Results', 'MergeGeno', 'temp', 'Merge')\n",
    "for filename in glob.glob(\"1000Ghg19.*\"):\n",
    "    shutil.copy(filename, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge reference samples\n",
    "\n",
    "Now we will merge the 1000G and HGDP databases, both using the hg19 coordinates and with related people removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(projpath, 'Results', 'MergeGeno', 'temp', 'Merge'))\n",
    "subprocess.run([\"plink\", \"--bfile\", \"1000Ghg19\", \"--bmerge\", \"hgdp940hg19\", \"--make-bed\", \"--out\", \"hgdp1000ghg19\"])\n",
    "for file in glob.glob(\"*.bed\"):\n",
    "    outname = file.split(\".\")[0] + \"_temp\"\n",
    "    subprocess.run([\"plink\", \"--bfile\", file.split(\".\")[0], \"--exclude\", \"hgdp1000ghg19-merge.missnp\", \"--make-bed\", \"--out\", outname])\n",
    "\n",
    "subprocess.run([\"plink\", \"--bfile\", \"hgdp940hg19_temp\", \"--bmerge\", \"1000Ghg19_temp\", \"--make-bed\", \"--out\", \"hgdp1000ghg19\"])\n",
    "for file in glob.glob(\"*_temp*\"):\n",
    "    os.remove(file)\n",
    "    \n",
    "dest_dir = os.path.join(projpath, 'Results', 'MergeGeno', 'MergeSamples', '05_ReferenceSamples')\n",
    "for filename in glob.glob(\"hgdp1000ghg19.*\"):\n",
    "    shutil.copy(filename, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging reference and in-house samples\n",
    "\n",
    "Finally, we will merge the in-house samples with the reference samples from HGDP and 1000G. \n",
    "To do that we will extract the snps from the in-house samples already pruned and cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLINK v1.90b4 64-bit (20 Mar 2017)\n",
      "Options in effect:\n",
      "  --bfile CleanMerged_subset_flip\n",
      "  --bmerge hgdp1000ghg19_subset\n",
      "  --make-bed\n",
      "  --out HouseHGDP1000Ghg19\n",
      "\n",
      "Hostname: tomasgazelle\n",
      "Working directory: /home/tomas/Downloads/FacialSD/Results/MergeGeno/MergeSamples/05_ReferenceSamples\n",
      "Start time: Mon May 14 11:57:17 2018\n",
      "\n",
      "Random number seed: 1526313437\n",
      "3865 MB RAM detected; reserving 1932 MB for main workspace.\n",
      "5290 people loaded from CleanMerged_subset_flip.fam.\n",
      "3444 people to be merged from hgdp1000ghg19_subset.fam.\n",
      "Of these, 3444 are new, while 0 are present in the base dataset.\n",
      "12890 markers loaded from CleanMerged_subset_flip.bim.\n",
      "12890 markers to be merged from hgdp1000ghg19_subset.bim.\n",
      "Of these, 0 are new, while 12890 are present in the base dataset.\n",
      "Performing single-pass merge (8734 people, 12890 variants).\n",
      "Merged fileset written to HouseHGDP1000Ghg19-merge.bed +\n",
      "HouseHGDP1000Ghg19-merge.bim + HouseHGDP1000Ghg19-merge.fam .\n",
      "12890 variants loaded from .bim file.\n",
      "8734 people (3723 males, 5011 females) loaded from .fam.\n",
      "Using 1 thread (no multithreaded calculations invoked).\n",
      "Before main variant filters, 8611 founders and 123 nonfounders present.\n",
      "Calculating allele frequencies... done.\n",
      "Total genotyping rate is 0.9865.\n",
      "12890 variants and 8734 people pass filters and QC.\n",
      "Note: No phenotypes present.\n",
      "--make-bed to HouseHGDP1000Ghg19.bed + HouseHGDP1000Ghg19.bim +\n",
      "HouseHGDP1000Ghg19.fam ... done.\n",
      "\n",
      "End time: Mon May 14 11:57:18 2018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.join(outhouse, \"05_ReferenceSamples\"))\n",
    "subprocess.run([\"plink\", \"--bfile\", \"hgdp1000ghg19\", \"--extract\", \"CleanMerged.bim\", \"--make-bed\", \"--out\", \"hgdp1000ghg19_subset\"])\n",
    "subprocess.run([\"plink\", \"--bfile\", \"CleanMerged\", \"--extract\", \"hgdp1000ghg19_subset.bim\", \"--make-bed\", \"--out\", \"CleanMerged_subset\"])\n",
    "subprocess.run([\"plink\", \"--bfile\", \"CleanMerged_subset\", \"--bmerge\", \"hgdp1000ghg19_subset\", \"--make-bed\", \"--out\", \"HouseHGDP1000Ghg19\"])\n",
    "#Fliping strand and merging\n",
    "subprocess.run([\"plink\", \"--bfile\", \"CleanMerged_subset\", \"--flip\", \"HouseHGDP1000Ghg19-merge.missnp\", \"--make-bed\", \"--out\", \"CleanMerged_subset_flip\"])\n",
    "subprocess.run([\"plink\", \"--bfile\", \"CleanMerged_subset_flip\", \"--bmerge\", \"hgdp1000ghg19_subset\", \"--make-bed\", \"--out\", \"HouseHGDP1000Ghg19\"])\n",
    "\n",
    "with open(\"HouseHGDP1000Ghg19.log\", 'r') as fin:\n",
    "    file_contents = fin.read()\n",
    "    print(file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population stratification\n",
    "\n",
    "Now, we will load the final dataset created before and run some populations stratification analyses (`PCA`, `MDS` and `ADMIXTURE`)\n",
    "\n",
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(outhouse, \"05_ReferenceSamples\"))\n",
    "subprocess.run([\"plink\", \"--bfile\", \"HouseHGDP1000Ghg19\", \"--pca\", \"50\", \"--pca-cluster-names\", \"0\", \"--within\", \"hgdp1000ghg19.fam\", \"--out\", \"PCA\"])\n",
    "for file in glob.glob(\"PCA.*\"):\n",
    "    shutil.move(file, os.path.join(projpath, \"Results\", \"GenPCA\", file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Admixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergesamples = os.path.join(outhouse, \"05_ReferenceSamples\", \"HouseHGDP1000Ghg19.bed\")\n",
    "os.chdir(os.path.join(projpath, \"Results\", \"Admixture\"))\n",
    "for i in range(2,11):\n",
    "    f = open(\"log_\" + str(i) + \".txt\", \"w\")\n",
    "    subprocess.run([\"./admixture\", \"--cv\", mergesamples, str(i)], stdout=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
